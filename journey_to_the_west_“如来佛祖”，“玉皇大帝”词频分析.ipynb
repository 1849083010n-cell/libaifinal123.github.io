{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvp0x9hqgixab0UhuRtRzO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1849083010n-cell/gdp-dashboard/blob/main/journey_to_the_west_%E2%80%9C%E5%A6%82%E6%9D%A5%E4%BD%9B%E7%A5%96%E2%80%9D%EF%BC%8C%E2%80%9C%E7%8E%89%E7%9A%87%E5%A4%A7%E5%B8%9D%E2%80%9D%E8%AF%8D%E9%A2%91%E5%88%86%E6%9E%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 连接Google云端硬盘\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 导入必要的库\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 设置中文字体，确保图表中文正常显示\n",
        "plt.rcParams[\"font.family\"] = [\"SimHei\", \"WenQuanYi Micro Hei\", \"Heiti TC\"]\n",
        "\n",
        "def count_and_visualize(file_path, keywords=['如來', '佛祖']):\n",
        "    \"\"\"\n",
        "    统计指定关键词的出现次数并生成柱状图\n",
        "\n",
        "    参数:\n",
        "        file_path: 文件在Google云端硬盘中的路径\n",
        "        keywords: 要统计的关键词列表，默认是['如來', '佛祖']\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 读取文件内容\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "\n",
        "        # 初始化计数字典\n",
        "        counts = {keyword: 0 for keyword in keywords}\n",
        "        # 存储每个次出现的上下文（可选展示）\n",
        "        contexts = {keyword: [] for keyword in keywords}\n",
        "\n",
        "        # 遍历每个关键词进行统计\n",
        "        for keyword in keywords:\n",
        "            pattern = re.compile(re.escape(keyword))  # 转义特殊字符\n",
        "            matches = pattern.finditer(content)\n",
        "\n",
        "            for match in matches:\n",
        "                counts[keyword] += 1\n",
        "                # 记录上下文（前后各30字符）\n",
        "                start = match.start()\n",
        "                end = match.end()\n",
        "                context_start = max(0, start - 15)\n",
        "                context_end = min(len(content), end + 15)\n",
        "                contexts[keyword].append(content[context_start:context_end])\n",
        "\n",
        "        # 打印统计结果\n",
        "        print(\"关键词出现次数统计：\")\n",
        "        for keyword, count in counts.items():\n",
        "            print(f\"'{keyword}' 共出现 {count} 次\")\n",
        "            # 可选：打印前3次出现的上下文（避免输出过多）\n",
        "            if counts[keyword] > 0:\n",
        "                print(f\"前3次出现的上下文示例：\")\n",
        "                for i, ctx in enumerate(contexts[keyword][:10]):\n",
        "                    print(f\"  第{i+1}次：...{ctx}...\")\n",
        "            print(\"-\" * 60)\n",
        "\n",
        "        # 生成柱状图\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        x = np.arange(len(keywords))\n",
        "        plt.bar(x, counts.values(), color=['#1f77b4', '#ff7f0e'])\n",
        "        plt.xticks(x, keywords, fontsize=12)\n",
        "        plt.ylabel('出现次数', fontsize=12)\n",
        "        plt.title('《西游记》中\"如來\"与\"佛祖\"出现次数对比', fontsize=14)\n",
        "\n",
        "        # 在柱形上标注具体数字\n",
        "        for i, v in enumerate(counts.values()):\n",
        "            plt.text(i, v + 5, str(v), ha='center', fontsize=10)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"错误：找不到文件 {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"发生错误：{str(e)}\")\n",
        "\n",
        "# 请修改为你的文件在Google云端硬盘中的实际路径\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/西遊記.txt'\n",
        "\n",
        "# 执行统计和可视化\n",
        "count_and_visualize(file_path)"
      ],
      "metadata": {
        "id": "o4D9UXbotNAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 连接Google云端硬盘\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 导入必要的库\n",
        "import re\n",
        "import jieba\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# 设置中文字体，确保中文正常显示\n",
        "plt.rcParams[\"font.family\"] = [\"SimHei\", \"WenQuanYi Micro Hei\", \"Heiti TC\"]\n",
        "\n",
        "def get_buddha_cooccurrence(file_path, target_words=['如來', '佛祖'], window=20, top_n=5):\n",
        "    \"\"\"\n",
        "    提取\"如來\"和\"佛祖\"出现时的高频共现词\n",
        "\n",
        "    参数:\n",
        "        file_path: 文件路径\n",
        "        target_words: 目标关键词（默认'如來'和'佛祖'）\n",
        "        window: 上下文窗口大小（前后各window个字符）\n",
        "        top_n: 显示前n个高频词\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 读取文件内容\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "\n",
        "        # 停用词列表（过滤无意义词汇，排除目标词本身）\n",
        "        stopwords = {'的', '了', '在', '是', '有', '就', '也', '着', '之', '于',\n",
        "                     '便', '乃', '与', '对', '向', '即', '等', '为', '所', '更',\n",
        "                     '如來', '佛祖', '佛', '世尊', '如来佛祖'}  # 排除相关称谓\n",
        "\n",
        "        # 存储每个目标词的共现词\n",
        "        cooccurrence = {word: [] for word in target_words}\n",
        "\n",
        "        for word in target_words:\n",
        "            # 匹配目标词\n",
        "            pattern = re.compile(re.escape(word))\n",
        "            matches = pattern.finditer(content)\n",
        "\n",
        "            for match in matches:\n",
        "                start, end = match.span()\n",
        "                # 提取上下文（前后各window个字符）\n",
        "                context_start = max(0, start - window)\n",
        "                context_end = min(len(content), end + window)\n",
        "                context = content[context_start:context_end]\n",
        "\n",
        "                # 分词并过滤停用词\n",
        "                words = jieba.lcut(context)  # 中文分词\n",
        "                # 保留长度>1的有效词汇，排除停用词\n",
        "                filtered = [w for w in words if w.strip() and w not in stopwords and len(w) > 1]\n",
        "                cooccurrence[word].extend(filtered)\n",
        "\n",
        "        # 统计高频词并可视化\n",
        "        for word in target_words:\n",
        "            if not cooccurrence[word]:\n",
        "                print(f\"未找到'{word}'的共现词\")\n",
        "                continue\n",
        "\n",
        "            # 统计词频并取前N\n",
        "            word_counts = Counter(cooccurrence[word])\n",
        "            top_words = word_counts.most_common(top_n)\n",
        "            top_words, counts = zip(*top_words) if top_words else ([], [])\n",
        "\n",
        "            # 打印结果\n",
        "            print(f\"\\n'{word}'出现时的前{top_n}高频共现词：\")\n",
        "            for w, c in zip(top_words, counts):\n",
        "                print(f\"  {w}: {c}次\")\n",
        "\n",
        "            # 生成柱状图\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            x = np.arange(len(top_words))\n",
        "            # 不同关键词用不同颜色区分\n",
        "            color = '#2ca02c' if word == '如來' else '#9467bd'\n",
        "            plt.bar(x, counts, color=color)\n",
        "            plt.xticks(x, top_words, fontsize=12)\n",
        "            plt.ylabel('共现次数', fontsize=12)\n",
        "            plt.title(f\"'{word}'的高频共现词（Top{top_n}）\", fontsize=14)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"错误：找不到文件 {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"发生错误：{str(e)}\")\n",
        "\n",
        "# 文件路径（请确认与你的《西游记》文本路径一致）\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/西遊記.txt'\n",
        "\n",
        "# 执行分析（提取\"如來\"和\"佛祖\"的前5高频共现词）\n",
        "get_buddha_cooccurrence(file_path)"
      ],
      "metadata": {
        "id": "0k2-_ohgtcnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 连接Google云端硬盘\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 导入必要的库\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 设置中文字体，确保图表中文正常显示\n",
        "plt.rcParams[\"font.family\"] = [\"SimHei\", \"WenQuanYi Micro Hei\", \"Heiti TC\"]\n",
        "\n",
        "def count_emperor_terms(file_path, keywords=['玉皇大帝', '玉皇', '玉帝', '大帝']):\n",
        "    \"\"\"\n",
        "    统计玉皇大帝相关称呼（含\"玉皇\"）的出现次数并生成可视化图表\n",
        "\n",
        "    参数:\n",
        "        file_path: 文件路径\n",
        "        keywords: 要统计的关键词列表（按包含关系排序）\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 读取文件内容\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "\n",
        "        # 初始化计数字典和上下文存储\n",
        "        counts = {kw: 0 for kw in keywords}\n",
        "        contexts = {kw: [] for kw in keywords}\n",
        "        # 记录已匹配的位置，避免重复计数\n",
        "        matched_positions = set()\n",
        "\n",
        "        # 按关键词长度排序（从长到短），优先匹配长关键词\n",
        "        sorted_keywords = sorted(keywords, key=lambda x: len(x), reverse=True)\n",
        "\n",
        "        for kw in sorted_keywords:\n",
        "            pattern = re.compile(re.escape(kw))\n",
        "            matches = pattern.finditer(content)\n",
        "\n",
        "            for match in matches:\n",
        "                start, end = match.span()\n",
        "                # 检查当前位置是否已被更长的关键词占用\n",
        "                if not any(start < pos_end and end > pos_start for pos_start, pos_end in matched_positions):\n",
        "                    counts[kw] += 1\n",
        "                    matched_positions.add((start, end))\n",
        "                    # 记录上下文（前后各30字符）\n",
        "                    context_start = max(0, start - 20)\n",
        "                    context_end = min(len(content), end + 20)\n",
        "                    contexts[kw].append(content[context_start:context_end])\n",
        "\n",
        "        # 打印统计结果\n",
        "        print(\"关键词出现次数统计：\")\n",
        "        for kw in keywords:  # 按原顺序输出\n",
        "            print(f\"'{kw}' 共出现 {counts[kw]} 次\")\n",
        "            # 显示前3次上下文示例\n",
        "            if counts[kw] > 0:\n",
        "                print(\"  前3次上下文示例：\")\n",
        "                for i, ctx in enumerate(contexts[kw][:10]):\n",
        "                    print(f\"    第{i+1}次：...{ctx}...\")\n",
        "            print(\"-\" * 70)\n",
        "\n",
        "        # 生成柱状图\n",
        "        plt.figure(figsize=(14, 7))\n",
        "        x = np.arange(len(keywords))\n",
        "        bars = plt.bar(x, counts.values(), color=['#2ca02c', '#98df8a', '#ffbb78', '#9467bd'])\n",
        "\n",
        "        # 设置坐标轴和标题\n",
        "        plt.xticks(x, keywords, fontsize=12)\n",
        "        plt.ylabel('出现次数', fontsize=12)\n",
        "        plt.title('《西游记》中玉皇大帝相关称呼出现次数对比', fontsize=14)\n",
        "\n",
        "        # 在柱形上标注具体数值\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            plt.text(bar.get_x() + bar.get_width()/2., height + 2,\n",
        "                    f'{height}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"错误：找不到文件 {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"发生错误：{str(e)}\")\n",
        "\n",
        "# 文件路径（请确认与你的《西游记》文本路径一致）\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/西遊記.txt'\n",
        "\n",
        "# 执行统计与可视化\n",
        "count_emperor_terms(file_path)"
      ],
      "metadata": {
        "id": "lni2xegEte3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 连接Google云端硬盘\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 导入必要的库\n",
        "import re\n",
        "import jieba\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# 设置中文字体\n",
        "plt.rcParams[\"font.family\"] = [\"SimHei\", \"WenQuanYi Micro Hei\", \"Heiti TC\"]\n",
        "\n",
        "def get_top_cooccurrence(file_path, target_words=['玉皇', '玉帝'], window=20, top_n=5):\n",
        "    \"\"\"\n",
        "    提取目标词出现时的高频共现词\n",
        "\n",
        "    参数:\n",
        "        file_path: 文件路径\n",
        "        target_words: 目标关键词（'玉皇'和'玉帝'）\n",
        "        window: 上下文窗口大小（前后各window个字符）\n",
        "        top_n: 显示前n个高频词\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 读取文件内容\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "\n",
        "        # 停用词列表（过滤无意义词汇）\n",
        "        stopwords = {'的', '了', '在', '是', '有', '就', '也', '着', '之', '于',\n",
        "                     '便', '乃', '与', '对', '向', '即', '等', '为', '所', '更',\n",
        "                     '玉皇', '玉帝', '玉皇大帝', '大帝'}  # 排除目标词本身\n",
        "\n",
        "        # 存储每个目标词的共现词\n",
        "        cooccurrence = {word: [] for word in target_words}\n",
        "\n",
        "        for word in target_words:\n",
        "            # 匹配目标词\n",
        "            pattern = re.compile(re.escape(word))\n",
        "            matches = pattern.finditer(content)\n",
        "\n",
        "            for match in matches:\n",
        "                start, end = match.span()\n",
        "                # 提取上下文（前后各window个字符）\n",
        "                context_start = max(0, start - window)\n",
        "                context_end = min(len(content), end + window)\n",
        "                context = content[context_start:context_end]\n",
        "\n",
        "                # 分词并过滤停用词\n",
        "                words = jieba.lcut(context)\n",
        "                filtered = [w for w in words if w.strip() and w not in stopwords and len(w) > 1]\n",
        "                cooccurrence[word].extend(filtered)\n",
        "\n",
        "        # 统计高频词并可视化\n",
        "        for word in target_words:\n",
        "            if not cooccurrence[word]:\n",
        "                print(f\"未找到'{word}'的共现词\")\n",
        "                continue\n",
        "\n",
        "            # 统计词频\n",
        "            word_counts = Counter(cooccurrence[word])\n",
        "            top_words = word_counts.most_common(top_n)\n",
        "            top_words, counts = zip(*top_words) if top_words else ([], [])\n",
        "\n",
        "            # 打印结果\n",
        "            print(f\"\\n'{word}'出现时的前{top_n}高频共现词：\")\n",
        "            for w, c in zip(top_words, counts):\n",
        "                print(f\"  {w}: {c}次\")\n",
        "\n",
        "            # 生成柱状图\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            x = np.arange(len(top_words))\n",
        "            plt.bar(x, counts, color=['#1f77b4' if word == '玉皇' else '#ff7f0e'])\n",
        "            plt.xticks(x, top_words, fontsize=12)\n",
        "            plt.ylabel('出现次数', fontsize=12)\n",
        "            plt.title(f\"'{word}'的高频共现词（Top{top_n}）\", fontsize=14)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"错误：找不到文件 {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"发生错误：{str(e)}\")\n",
        "\n",
        "# 文件路径（请确认与你的《西游记》文本路径一致）\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/西遊記.txt'\n",
        "\n",
        "# 执行分析（提取\"玉皇\"和\"玉帝\"的前5高频共现词）\n",
        "get_top_cooccurrence(file_path)"
      ],
      "metadata": {
        "id": "NUiAvkdntpaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3g0x5UlBsuC9"
      },
      "outputs": [],
      "source": [
        "# 步骤1：挂载Google Drive（已挂载可跳过）\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 安装依赖\n",
        "!pip install pyvis -q\n",
        "!pip install jieba -q\n",
        "\n",
        "# 步骤2：读取繁体《西游记》文本\n",
        "import re\n",
        "\n",
        "# 读取繁体文本并预处理\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/西遊記.txt'\n",
        "try:\n",
        "    # 尝试多种编码读取繁体文件\n",
        "    encodings = ['utf-8', 'big5', 'utf-8-sig']\n",
        "    text = None\n",
        "    for encoding in encodings:\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding=encoding) as f:\n",
        "                text = f.read()\n",
        "            break\n",
        "        except UnicodeDecodeError:\n",
        "            continue\n",
        "    if text is None:\n",
        "        raise Exception(\"无法解析文件编码（繁体文件建议用utf-8或big5）\")\n",
        "\n",
        "    # 文本清洗\n",
        "    text = re.sub(r'[^\\u4e00-\\u9fa5，。！？、,.:;!?\\n ]', '', text)\n",
        "    text = re.sub(r'\\n+', '\\n', text).strip()\n",
        "    print(f\"繁体文本读取成功！长度：{len(text)}字符\")\n",
        "    print(\"文本预览（前200字）：\", text[:200])\n",
        "except Exception as e:\n",
        "    print(f\"文本处理失败：{e}\")\n",
        "\n",
        "# 步骤3：定义繁体角色及别名\n",
        "character_aliases = {\n",
        "    '孫悟空': ['孫悟空', '孫行者', '大聖', '齊天大聖', '美猴王', '悟空'],\n",
        "    '唐僧': ['唐僧', '唐三藏', '玄奘', '師父'],\n",
        "    '豬八戒': ['豬八戒', '八戒', '豬悟能', '呆子'],\n",
        "    '沙和尚': ['沙和尚', '沙僧', '沙悟淨', '沙師弟'],\n",
        "    '白龍馬': ['白龍馬', '白馬'],\n",
        "    '玉皇大帝': ['玉皇大帝', '玉帝', '玉皇'],\n",
        "    '如來佛祖': ['如來佛祖', '如來', '佛祖'],\n",
        "    '觀音菩薩': ['觀音菩薩', '觀音', '觀世音'],\n",
        "    '牛魔王': ['牛魔王', '牛魔'],\n",
        "    '鐵扇公主': ['鐵扇公主', '鐵扇仙', '羅剎女'],\n",
        "    '紅孩兒': ['紅孩兒', '聖嬰大王'],\n",
        "    '太上老君': ['太上老君', '老君'],\n",
        "    '白骨精': ['白骨精', '白骨夫人'],\n",
        "    '二郎神': ['二郎神', '楊戩'],\n",
        "    '哪吒': ['哪吒', '三壇海會大神'],\n",
        "    '菩提祖師': ['菩提祖師', '菩提老祖'],\n",
        "    '四海龍王': ['四海龍王', '東海龍王', '西海龍王', '南海龍王', '北海龍王'],\n",
        "    '鎮元子': ['鎮元子', '鎮元大仙'],\n",
        "    '女兒國國王': ['女兒國國王', '女王']\n",
        "}\n",
        "\n",
        "# 构建别名映射\n",
        "alias_to_name = {}\n",
        "for std_name, aliases in character_aliases.items():\n",
        "    for alias in aliases:\n",
        "        alias_to_name[alias] = std_name\n",
        "\n",
        "all_aliases = [alias for aliases in character_aliases.values() for alias in aliases]\n",
        "\n",
        "# 步骤4：提取关系\n",
        "def extract_traditional_relations(text):\n",
        "    relations = {}\n",
        "    paragraphs = text.split('\\n')\n",
        "\n",
        "    for para in paragraphs:\n",
        "        has_sun = any(alias in para for alias in character_aliases['孫悟空'])\n",
        "        if not has_sun:\n",
        "            continue\n",
        "\n",
        "        for alias in all_aliases:\n",
        "            if alias in character_aliases['孫悟空']:\n",
        "                continue\n",
        "            if alias in para:\n",
        "                target_name = alias_to_name[alias]\n",
        "                para_lower = para.lower()\n",
        "\n",
        "                if target_name == '唐僧' and ('師父' in para or '三藏' in para):\n",
        "                    rel_type = '師徒（孫悟空→唐僧）'\n",
        "                elif target_name in ['豬八戒', '沙和尚'] and ('師弟' in para):\n",
        "                    rel_type = '師兄弟'\n",
        "                elif '打' in para or '殺' in para or '敵' in para or '妖' in para:\n",
        "                    rel_type = '敵對'\n",
        "                elif '幫' in para or '救' in para or '助' in para:\n",
        "                    rel_type = '友好'\n",
        "                elif '佛' in target_name or '菩薩' in target_name or '仙' in target_name:\n",
        "                    rel_type = '神仙-弟子'\n",
        "                else:\n",
        "                    rel_type = '互動'\n",
        "\n",
        "                if target_name in relations:\n",
        "                    relations[target_name] = (relations[target_name][0], relations[target_name][1] + 1)\n",
        "                else:\n",
        "                    relations[target_name] = (rel_type, 1)\n",
        "\n",
        "    relation_list = [('孫悟空', k, v[0], v[1]) for k, v in relations.items()]\n",
        "    return sorted(relation_list, key=lambda x: x[3], reverse=True)\n",
        "\n",
        "# 执行关系提取\n",
        "if 'text' in locals():\n",
        "    sun_relations = extract_traditional_relations(text)\n",
        "    print(\"\\n提取到的繁体角色关系（按互动次数排序）：\")\n",
        "    if sun_relations:\n",
        "        for rel in sun_relations[:15]:\n",
        "            print(f\"{rel[0]} 與 {rel[1]}：{rel[2]}（{rel[3]}次）\")\n",
        "    else:\n",
        "        print(\"未提取到关系，可能是繁体别名未覆盖文本中的表述\")\n",
        "\n",
        "# 步骤5：绘制关系网络图（修正字体设置错误）\n",
        "from pyvis.network import Network\n",
        "from IPython.display import HTML\n",
        "\n",
        "# 修正后的网络图初始化代码（关键是加了 cdn_resources='in_line'）\n",
        "def draw_traditional_network(relations):\n",
        "    if not relations:\n",
        "        return \"無關係數據可繪製\"\n",
        "\n",
        "    # 这里添加 cdn_resources='in_line'，解决浏览器显示问题\n",
        "    net = Network(\n",
        "        height=\"700px\", width=\"100%\",\n",
        "        bgcolor=\"#ffffff\", font_color=\"#333333\",\n",
        "        notebook=True,\n",
        "        cdn_resources='in_line'  # 新增这行，解决Chrome/Safari显示问题\n",
        "    )\n",
        "\n",
        "    # 后面的添加节点、边的代码不变...\n",
        "\n",
        "    # 添加孫悟空节点（在font中设置字体）\n",
        "    net.add_node(\n",
        "        \"孫悟空\",\n",
        "        size=60,\n",
        "        color=\"#FFD700\",\n",
        "        title=\"主角：孫悟空（美猴王、齊天大聖）\",\n",
        "        font={\"size\": 20, \"color\": \"#000000\", \"face\": \"Microsoft YaHei\"}  # 这里设置字体\n",
        "    )\n",
        "\n",
        "    # 添加其他节点（同样在font中设置字体）\n",
        "    for rel in relations:\n",
        "        source, target, rel_type, count = rel\n",
        "        node_size = min(20 + count // 3, 50)\n",
        "\n",
        "        if \"敵對\" in rel_type:\n",
        "            node_color = \"#FF6347\"\n",
        "        elif \"師徒\" in rel_type or \"師兄弟\" in rel_type:\n",
        "            node_color = \"#32CD32\"\n",
        "        elif \"神仙\" in rel_type:\n",
        "            node_color = \"#1E90FF\"\n",
        "        else:\n",
        "            node_color = \"#9370DB\"\n",
        "\n",
        "        net.add_node(\n",
        "            target,\n",
        "            size=node_size,\n",
        "            color=node_color,\n",
        "            title=f\"{target}\\n互動次數：{count}\",\n",
        "            font={\"size\": 14, \"face\": \"Microsoft YaHei\"}  # 节点字体设置\n",
        "        )\n",
        "\n",
        "        net.add_edge(\n",
        "            source, target,\n",
        "            label=rel_type,\n",
        "            width=min(count // 5, 10),\n",
        "            color=\"#888888\",\n",
        "            title=f\"互動次數：{count}\",\n",
        "            font={\"face\": \"Microsoft YaHei\"}  # 边的标签字体设置\n",
        "        )\n",
        "\n",
        "    # 布局设置\n",
        "    net.set_options(\"\"\"\n",
        "    {\n",
        "        \"physics\": {\n",
        "            \"barnesHut\": {\n",
        "                \"gravitationalConstant\": -2200,\n",
        "                \"springLength\": 180\n",
        "            },\n",
        "            \"stabilization\": {\n",
        "                \"iterations\": 250,\n",
        "                \"fit\": true\n",
        "            }\n",
        "        },\n",
        "        \"edges\": {\n",
        "            \"smooth\": {\n",
        "                \"type\": \"cubicBezier\",\n",
        "                \"forceDirection\": \"horizontal\"\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    \"\"\")\n",
        "\n",
        "    net.save_graph(\"sun_wukong_traditional_network.html\")\n",
        "    return HTML(\"sun_wukong_traditional_network.html\")\n",
        "\n",
        "# 绘制网络图\n",
        "if 'sun_relations' in locals() and sun_relations:\n",
        "    print(\"\\n正在繪製繁体角色關係網絡圖...\")\n",
        "    display(draw_traditional_network(sun_relations))\n",
        "else:\n",
        "    print(\"\\n無法繪製網絡圖（無關係數據）\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# 手动安装中文字体\n",
        "!apt-get update -qq\n",
        "!apt-get install -y fonts-wqy-microhei\n",
        "!fc-cache -fv\n",
        "import re\n",
        "import jieba\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "# 配置中文字体\n",
        "plt.rcParams[\"font.family\"] = [\"SimHei\", \"WenQuanYi Micro Hei\", \"Heiti TC\", \"Arial Unicode MS\"]\n",
        "plt.rcParams[\"axes.unicode_minus\"] = False\n",
        "try:\n",
        "    font_path = [f for f in fm.findSystemFonts() if 'unicode' in f.lower() or 'heiti' in f.lower()][0]\n",
        "    font_prop = fm.FontProperties(fname=font_path)\n",
        "    plt.rcParams[\"font.family\"] = font_prop.get_name()\n",
        "except:\n",
        "    plt.rcParams[\"font.family\"] = \"Arial Unicode MS\"\n",
        "\n",
        "def analyze_buddha_sentiment(file_path):\n",
        "    \"\"\"分析《西游记》中如来的出现情况及情感属性\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "\n",
        "        # 章节分割\n",
        "        chapter_pattern = re.compile(r'第[\\u4e00-\\u9fa5零一二三四五六七八九十百千\\d]+回[：: ]?')\n",
        "        chapter_matches = list(chapter_pattern.finditer(content))\n",
        "\n",
        "        if not chapter_matches:\n",
        "            print(\"警告：未找到章节标记，将全文视为一个章节\")\n",
        "            chapters = [content]\n",
        "            chapter_titles = [\"全文\"]\n",
        "        else:\n",
        "            chapter_titles = [match.group().strip() for match in chapter_matches]\n",
        "            chapters = []\n",
        "            for i in range(len(chapter_matches)):\n",
        "                start = chapter_matches[i].end()\n",
        "                end = chapter_matches[i+1].start() if i < len(chapter_matches)-1 else len(content)\n",
        "                chapters.append(content[start:end])\n",
        "\n",
        "        # 情感分析词库（增加权重维度）\n",
        "        sentiment_words = {\n",
        "            'positive': {\n",
        "                '善': 2, '慈悲': 3, '智慧': 3, '功德': 2, '救': 2,\n",
        "                '助': 2, '圣': 3, '尊': 2, '贤': 2, '佛法': 3,\n",
        "                '真经': 3, '觉悟': 2, '解脱': 2, '圆满': 3\n",
        "            },\n",
        "            'negative': {\n",
        "                '怪': 2, '妖': 2, '魔': 3, '难': 2, '苦': 2,\n",
        "                '斗': 2, '敌': 2, '恶': 3, '孽': 3, '罚': 2,\n",
        "                '恼': 1, '骗': 2, '害': 2, '乱': 2\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # 统计各章节如来出现及情感倾向\n",
        "        buddha_data = []\n",
        "        for idx, (title, chapter) in enumerate(zip(chapter_titles, chapters)):\n",
        "            # 如来出现次数统计\n",
        "            count = len(re.findall(r'如來佛祖|如来佛祖|如来', chapter))\n",
        "\n",
        "            # 提取如来上下文\n",
        "            context_pattern = re.compile(r'(.{0,30}如來.{0,30}|.{0,30}如来.{0,30})')\n",
        "            contexts = context_pattern.findall(chapter)\n",
        "            context_text = ' '.join(contexts)\n",
        "\n",
        "            # 情感得分计算\n",
        "            words = jieba.cut(context_text)\n",
        "            pos_score = 0\n",
        "            neg_score = 0\n",
        "            pos_words = []\n",
        "            neg_words = []\n",
        "\n",
        "            for word in words:\n",
        "                if word in sentiment_words['positive']:\n",
        "                    pos_score += sentiment_words['positive'][word]\n",
        "                    pos_words.append(word)\n",
        "                elif word in sentiment_words['negative']:\n",
        "                    neg_score += sentiment_words['negative'][word]\n",
        "                    neg_words.append(word)\n",
        "\n",
        "            # 净情感值（正面减负面）\n",
        "            net_sentiment = pos_score - neg_score\n",
        "\n",
        "            buddha_data.append({\n",
        "                '章节': title,\n",
        "                '章节索引': idx + 1,\n",
        "                '出现次数': count,\n",
        "                '正面得分': pos_score,\n",
        "                '负面得分': neg_score,\n",
        "                '净情感值': net_sentiment,\n",
        "                '正面词汇': Counter(pos_words).most_common(3),\n",
        "                '负面词汇': Counter(neg_words).most_common(3)\n",
        "            })\n",
        "\n",
        "        # 创建数据框并展示\n",
        "        df = pd.DataFrame(buddha_data)\n",
        "        non_zero_df = df[df['出现次数'] > 0].sort_values(by='章节索引')\n",
        "        print(\"如来出现章节及情感分析：\")\n",
        "        print(non_zero_df[['章节', '出现次数', '正面得分', '负面得分', '净情感值']])\n",
        "\n",
        "        # 可视化1：出现次数与净情感值趋势\n",
        "        if not non_zero_df.empty:\n",
        "            fig, ax1 = plt.subplots(figsize=(15, 8))\n",
        "            ax2 = ax1.twinx()\n",
        "\n",
        "            ax1.bar(non_zero_df['章节索引'], non_zero_df['出现次数'], color='skyblue', alpha=0.6, label='出现次数')\n",
        "            ax2.plot(non_zero_df['章节索引'], non_zero_df['净情感值'], color='red', marker='o', label='净情感值')\n",
        "\n",
        "            ax1.set_xlabel('章节索引')\n",
        "            ax1.set_ylabel('出现次数', color='blue')\n",
        "            ax2.set_ylabel('净情感值', color='red')\n",
        "            plt.title('如来出现次数与情感倾向变化趋势')\n",
        "            fig.legend(loc='upper right')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        # 提取所有相关上下文进行整体情感分析\n",
        "        pattern = re.compile(r'(.{0,50}如來.{0,50}|.{0,50}如来.{0,50})')\n",
        "        all_contexts = pattern.findall(content)\n",
        "        if not all_contexts:\n",
        "            print(\"未找到包含'如来'的内容\")\n",
        "            return\n",
        "\n",
        "        all_text = ' '.join(all_contexts)\n",
        "        stopwords = {'的', '了', '在', '是', '他', '就', '有', '也', '和', '之', '为', '着', '于', '这', '那', '曰', '道', '说'}\n",
        "        words = jieba.cut(all_text)\n",
        "        filtered_words = [word for word in words if len(word) > 1 and word not in stopwords and word not in ['如來', '如来']]\n",
        "\n",
        "        # 整体情感词汇统计\n",
        "        total_pos = Counter()\n",
        "        total_neg = Counter()\n",
        "        for word in filtered_words:\n",
        "            if word in sentiment_words['positive']:\n",
        "                total_pos[word] += sentiment_words['positive'][word]\n",
        "            elif word in sentiment_words['negative']:\n",
        "                total_neg[word] += sentiment_words['negative'][word]\n",
        "\n",
        "        # 可视化2：情感词汇分布\n",
        "        plt.figure(figsize=(14, 6))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        pos_df = pd.DataFrame(total_pos.most_common(10), columns=['词汇', '权重得分'])\n",
        "        plt.barh(pos_df['词汇'], pos_df['权重得分'], color='green')\n",
        "        plt.title('高频正面情感词汇（带权重）')\n",
        "        plt.gca().invert_yaxis()\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        neg_df = pd.DataFrame(total_neg.most_common(10), columns=['词汇', '权重得分'])\n",
        "        plt.barh(neg_df['词汇'], neg_df['权重得分'], color='red')\n",
        "        plt.title('高频负面情感词汇（带权重）')\n",
        "        plt.gca().invert_yaxis()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # 情感总结\n",
        "        total_pos_score = sum(total_pos.values())\n",
        "        total_neg_score = sum(total_neg.values())\n",
        "        sentiment_ratio = total_pos_score / (total_neg_score + 1)  # 避免除零\n",
        "\n",
        "        print(\"\\n===== 情感属性总结 =====\")\n",
        "        print(f\"整体正面情感得分：{total_pos_score}\")\n",
        "        print(f\"整体负面情感得分：{total_neg_score}\")\n",
        "        print(f\"正负面情感比率：{sentiment_ratio:.2f}\")\n",
        "        print(\"\\n主要正面词汇：\", [w[0] for w in total_pos.most_common(5)])\n",
        "        print(\"主要负面词汇：\", [w[0] for w in total_neg.most_common(5)])\n",
        "\n",
        "        # 可视化3：整体情感倾向\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.pie(\n",
        "            [total_pos_score, total_neg_score],\n",
        "            labels=['正面情感', '负面情感'],\n",
        "            autopct='%1.1f%%',\n",
        "            colors=['green', 'red'],\n",
        "            startangle=90\n",
        "        )\n",
        "        plt.title('如来相关内容整体情感倾向分布')\n",
        "        plt.show()\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"错误：找不到文件 {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"发生错误：{str(e)}\")\n",
        "\n",
        "# 设置文件路径（请根据实际路径修改）\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/西遊記.txt'\n",
        "\n",
        "# 执行分析\n",
        "analyze_buddha_sentiment(file_path)"
      ],
      "metadata": {
        "id": "CNnWF0dGs-f6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# 手动安装中文字体\n",
        "!apt-get update -qq\n",
        "!apt-get install -y fonts-wqy-microhei\n",
        "!fc-cache -fv\n",
        "import re\n",
        "import jieba\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "# 配置中文字体\n",
        "plt.rcParams[\"font.family\"] = [\"SimHei\", \"WenQuanYi Micro Hei\", \"Heiti TC\", \"Arial Unicode MS\"]\n",
        "plt.rcParams[\"axes.unicode_minus\"] = False\n",
        "try:\n",
        "    font_path = [f for f in fm.findSystemFonts() if 'unicode' in f.lower() or 'heiti' in f.lower()][0]\n",
        "    font_prop = fm.FontProperties(fname=font_path)\n",
        "    plt.rcParams[\"font.family\"] = font_prop.get_name()\n",
        "except:\n",
        "    plt.rcParams[\"font.family\"] = \"Arial Unicode MS\"\n",
        "\n",
        "def analyze_yuandi_sentiment(file_path):\n",
        "    \"\"\"分析《西游记》中玉帝的出现情况及情感属性\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "\n",
        "        # 章节分割\n",
        "        chapter_pattern = re.compile(r'第[\\u4e00-\\u9fa5零一二三四五六七八九十百千\\d]+回[：: ]?')\n",
        "        chapter_matches = list(chapter_pattern.finditer(content))\n",
        "\n",
        "        if not chapter_matches:\n",
        "            print(\"警告：未找到章节标记，将全文视为一个章节\")\n",
        "            chapters = [content]\n",
        "            chapter_titles = [\"全文\"]\n",
        "        else:\n",
        "            chapter_titles = [match.group().strip() for match in chapter_matches]\n",
        "            chapters = []\n",
        "            for i in range(len(chapter_matches)):\n",
        "                start = chapter_matches[i].end()\n",
        "                end = chapter_matches[i+1].start() if i < len(chapter_matches)-1 else len(content)\n",
        "                chapters.append(content[start:end])\n",
        "\n",
        "        # 情感分析词库（适配玉帝角色的词汇体系）\n",
        "        sentiment_words = {\n",
        "            'positive': {\n",
        "                '仁': 3, '圣明': 3, '威严': 2, '公正': 3, '恩德': 2,\n",
        "                '仁慈': 3, '英明': 3, '功德': 2, '敕令': 2, '天恩': 3,\n",
        "                '神圣': 2, '尊贵': 3, '威严': 2, '公正': 3\n",
        "            },\n",
        "            'negative': {\n",
        "                '怒': 2, '惊': 2, '慌': 2, '无奈': 3, '难': 2,\n",
        "                '急': 2, '怕': 3, '愁': 2, '乱': 2, '束手无策': 3,\n",
        "                '狼狈': 3, '失色': 2, '无能': 3, '窘': 2\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # 统计各章节玉帝出现及情感倾向\n",
        "        yuandi_data = []\n",
        "        for idx, (title, chapter) in enumerate(zip(chapter_titles, chapters)):\n",
        "            # 玉帝相关称呼匹配（包含玉皇大帝、大帝等）\n",
        "            count = len(re.findall(r'玉帝|玉皇大帝|大帝|玉皇|玉皇帝', chapter))\n",
        "\n",
        "            # 提取玉帝上下文\n",
        "            context_pattern = re.compile(r'(.{0,30}玉帝.{0,30}|.{0,30}玉皇大帝.{0,30}|.{0,30}大帝.{0,30}|.{0,30}玉皇.{0,30})')\n",
        "            contexts = context_pattern.findall(chapter)\n",
        "            context_text = ' '.join(contexts)\n",
        "\n",
        "            # 情感得分计算\n",
        "            words = jieba.cut(context_text)\n",
        "            pos_score = 0\n",
        "            neg_score = 0\n",
        "            pos_words = []\n",
        "            neg_words = []\n",
        "\n",
        "            for word in words:\n",
        "                if word in sentiment_words['positive']:\n",
        "                    pos_score += sentiment_words['positive'][word]\n",
        "                    pos_words.append(word)\n",
        "                elif word in sentiment_words['negative']:\n",
        "                    neg_score += sentiment_words['negative'][word]\n",
        "                    neg_words.append(word)\n",
        "\n",
        "            # 净情感值（正面减负面）\n",
        "            net_sentiment = pos_score - neg_score\n",
        "\n",
        "            yuandi_data.append({\n",
        "                '章节': title,\n",
        "                '章节索引': idx + 1,\n",
        "                '出现次数': count,\n",
        "                '正面得分': pos_score,\n",
        "                '负面得分': neg_score,\n",
        "                '净情感值': net_sentiment,\n",
        "                '正面词汇': Counter(pos_words).most_common(3),\n",
        "                '负面词汇': Counter(neg_words).most_common(3)\n",
        "            })\n",
        "\n",
        "        # 创建数据框并展示\n",
        "        df = pd.DataFrame(yuandi_data)\n",
        "        non_zero_df = df[df['出现次数'] > 0].sort_values(by='章节索引')\n",
        "        print(\"玉帝出现章节及情感分析：\")\n",
        "        print(non_zero_df[['章节', '出现次数', '正面得分', '负面得分', '净情感值']])\n",
        "\n",
        "        # 可视化1：出现次数与净情感值趋势\n",
        "        if not non_zero_df.empty:\n",
        "            fig, ax1 = plt.subplots(figsize=(15, 8))\n",
        "            ax2 = ax1.twinx()\n",
        "\n",
        "            ax1.bar(non_zero_df['章节索引'], non_zero_df['出现次数'], color='gold', alpha=0.6, label='出现次数')\n",
        "            ax2.plot(non_zero_df['章节索引'], non_zero_df['净情感值'], color='purple', marker='o', label='净情感值')\n",
        "\n",
        "            ax1.set_xlabel('章节索引')\n",
        "            ax1.set_ylabel('出现次数', color='gold')\n",
        "            ax2.set_ylabel('净情感值', color='purple')\n",
        "            plt.title('玉帝出现次数与情感倾向变化趋势')\n",
        "            fig.legend(loc='upper right')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        # 提取所有相关上下文进行整体情感分析\n",
        "        pattern = re.compile(r'(.{0,50}玉帝.{0,50}|.{0,50}玉皇大帝.{0,50}|.{0,50}大帝.{0,50}|.{0,50}玉皇.{0,50})')\n",
        "        all_contexts = pattern.findall(content)\n",
        "        if not all_contexts:\n",
        "            print(\"未找到包含'玉帝'的内容\")\n",
        "            return\n",
        "\n",
        "        all_text = ' '.join(all_contexts)\n",
        "        stopwords = {'的', '了', '在', '是', '他', '就', '有', '也', '和', '之', '为', '着', '于', '这', '那', '曰', '道', '说', '天', '宫'}\n",
        "        words = jieba.cut(all_text)\n",
        "        filtered_words = [word for word in words if len(word) > 1 and word not in stopwords and word not in ['玉帝', '玉皇大帝', '大帝', '玉皇']]\n",
        "\n",
        "        # 整体情感词汇统计\n",
        "        total_pos = Counter()\n",
        "        total_neg = Counter()\n",
        "        for word in filtered_words:\n",
        "            if word in sentiment_words['positive']:\n",
        "                total_pos[word] += sentiment_words['positive'][word]\n",
        "            elif word in sentiment_words['negative']:\n",
        "                total_neg[word] += sentiment_words['negative'][word]\n",
        "\n",
        "        # 可视化2：情感词汇分布\n",
        "        plt.figure(figsize=(14, 6))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        pos_df = pd.DataFrame(total_pos.most_common(10), columns=['词汇', '权重得分'])\n",
        "        plt.barh(pos_df['词汇'], pos_df['权重得分'], color='green')\n",
        "        plt.title('高频正面情感词汇（带权重）')\n",
        "        plt.gca().invert_yaxis()\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        neg_df = pd.DataFrame(total_neg.most_common(10), columns=['词汇', '权重得分'])\n",
        "        plt.barh(neg_df['词汇'], neg_df['权重得分'], color='red')\n",
        "        plt.title('高频负面情感词汇（带权重）')\n",
        "        plt.gca().invert_yaxis()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # 情感总结\n",
        "        total_pos_score = sum(total_pos.values())\n",
        "        total_neg_score = sum(total_neg.values())\n",
        "        sentiment_ratio = total_pos_score / (total_neg_score + 1)  # 避免除零\n",
        "\n",
        "        print(\"\\n===== 情感属性总结 =====\")\n",
        "        print(f\"整体正面情感得分：{total_pos_score}\")\n",
        "        print(f\"整体负面情感得分：{total_neg_score}\")\n",
        "        print(f\"正负面情感比率：{sentiment_ratio:.2f}\")\n",
        "        print(\"\\n主要正面词汇：\", [w[0] for w in total_pos.most_common(5)])\n",
        "        print(\"主要负面词汇：\", [w[0] for w in total_neg.most_common(5)])\n",
        "\n",
        "        # 可视化3：整体情感倾向\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.pie(\n",
        "            [total_pos_score, total_neg_score],\n",
        "            labels=['正面情感', '负面情感'],\n",
        "            autopct='%1.1f%%',\n",
        "            colors=['green', 'red'],\n",
        "            startangle=90\n",
        "        )\n",
        "        plt.title('玉帝相关内容整体情感倾向分布')\n",
        "        plt.show()\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"错误：找不到文件 {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"发生错误：{str(e)}\")\n",
        "\n",
        "# 设置文件路径（请根据实际路径修改）\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/西遊記.txt'\n",
        "\n",
        "# 执行分析\n",
        "analyze_yuandi_sentiment(file_path)"
      ],
      "metadata": {
        "id": "BuxEbZyctFkw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}